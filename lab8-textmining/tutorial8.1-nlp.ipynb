{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beb928c5",
   "metadata": {},
   "source": [
    "# Tutorial 8-1. Natural Language Processing (NLP)\n",
    "\n",
    "**GOAL**: Let's taste how to process natural language text using Python!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40a2644",
   "metadata": {},
   "source": [
    "### 0. Installation\n",
    "\n",
    "Before starting this tutorial, please be prepared by installing the two packages.\n",
    "- English: `nltk` package\n",
    "- Korean: `konlpy` package\n",
    "\n",
    "You can install them by running the following lines in Anaconda Prompt:\n",
    "```\n",
    ">> conda install nltk\n",
    ">> pip install konlpy\n",
    "```\n",
    "\n",
    "If you are using Windows OS, you have to install JDK (see slides)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089f620b",
   "metadata": {},
   "source": [
    "### 1. English NLP using NLTK package\n",
    "\n",
    "[NLTK](https://www.nltk.org/) is a pioneering NLP package built in Python.\n",
    "\n",
    "First, import the `nltk` package and download a tokenizer and a pos tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d23b6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gugu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/gugu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt') # tokenizer\n",
    "nltk.download('averaged_perceptron_tagger') # pos tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c16015",
   "metadata": {},
   "source": [
    "Try your own sentences. You can tokenize, POS tag, or extract nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "164879be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['by', 'the', 'end', 'of', 'this', 'course', 'i', 'will', 'be', 'a', 'great', 'data', 'scientist', '!', '100', '%', 'sure', '!']\n",
      "[('by', 'IN'), ('the', 'DT'), ('end', 'NN'), ('of', 'IN'), ('this', 'DT'), ('course', 'NN'), ('i', 'NN'), ('will', 'MD'), ('be', 'VB'), ('a', 'DT'), ('great', 'JJ'), ('data', 'NNS'), ('scientist', 'NN'), ('!', '.'), ('100', 'CD'), ('%', 'NN'), ('sure', 'JJ'), ('!', '.')]\n",
      "['end', 'course', 'i', 'data', 'scientist', '%']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"By the end of this course I will be a great data scientist! 100% sure!\"\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence.lower()) # lowercase\n",
    "print(tokens)\n",
    "\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(tagged)\n",
    "\n",
    "nouns = [word for word, tag in tagged if tag[:2] == 'NN']\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f32beaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f91b672",
   "metadata": {},
   "source": [
    "### 2. Korean NLP using KoNLPy pacakge\n",
    "\n",
    "[KoNLPy](https://konlpy.org/) is a Python package for NLP of Korean language.\n",
    "\n",
    "It contains Hannanum (`Hannanum`), Kkma(`Kkma`), and Open Korean Text (`Okt`, aka Twitter) POS taggers.\n",
    "\n",
    "Here, we will use Twitter tagger. Let's load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7c50153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7353c117",
   "metadata": {},
   "source": [
    "Try your own sentence. You can tokenize, POS tag, or extract nouns and phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "920d975d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['데이터', '분석', '수업', '이', '넘나', '재밌어서', '현기증', '이', '나요', '...', 'ㅋ', '_', 'ㅠ']\n",
      "[('데이터', 'Noun'), ('분석', 'Noun'), ('수업', 'Noun'), ('이', 'Josa'), ('넘나', 'Verb'), ('재밌어서', 'Adjective'), ('현기증', 'Noun'), ('이', 'Josa'), ('나요', 'Eomi'), ('...', 'Punctuation'), ('ㅋ', 'KoreanParticle'), ('_', 'Punctuation'), ('ㅠ', 'KoreanParticle')]\n",
      "['데이터', '분석', '수업', '현기증']\n",
      "['데이터', '데이터 분석', '데이터 분석 수업', '현기증', '분석', '수업']\n"
     ]
    }
   ],
   "source": [
    "sentence = '데이터 분석 수업이 넘나 재밌어서 현기증이 나요...ㅋ_ㅠ'\n",
    "# sentence = '공부를 하면할수록 모르는게 많다는 것을 알게 됩니다. 배운건 많았는데... 다 까먹어버렸네요? ㅋㅋ 그래도 계속 공부합니다. 재밌으니까!'\n",
    "\n",
    "tokens = okt.morphs(sentence)\n",
    "print(tokens)\n",
    "\n",
    "tagged = okt.pos(sentence)\n",
    "print(tagged)\n",
    "\n",
    "nouns = okt.nouns(sentence)\n",
    "print(nouns)\n",
    "\n",
    "phrases = okt.phrases(sentence)\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9e89f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_ipykernel",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
